{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from time import strftime\n",
    "from calendar import monthrange\n",
    "import operator\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as datareader\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, GridSearchCV\n",
    "\n",
    "import xgboost\n",
    "\n",
    "import plotly as py\n",
    "print (py.__version__) # requires version >= 1.9.0\\n\",\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as ff\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>TB3MS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1963-02-28</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1963-03-31</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1963-04-30</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1963-05-31</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DATETIME  TB3MS\n",
       "0 1963-01-31   2.91\n",
       "1 1963-02-28   2.92\n",
       "2 1963-03-31   2.89\n",
       "3 1963-04-30   2.90\n",
       "4 1963-05-31   2.93"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_datestr(datestr):\n",
    "    \"\"\"Convert a beginning of month date string to end of month\n",
    "    2018-01-01 -> 2018-01-31\"\"\"\n",
    "    year_str, month_str, day_str = datestr.split('-')\n",
    "    end_of_month = monthrange(int(year_str), int(month_str))[1]\n",
    "    return \"%s-%s-%02d\" % (year_str, month_str, end_of_month)\n",
    "\n",
    "def get_fred_data(series, start_date, end_date):\n",
    "    retframe = datareader.DataReader(series, \"fred\", start_date, end_date)\n",
    "    # convert dates from start to end of month and set index\n",
    "    retframe['yyyymmdd'] = retframe.index.strftime('%Y-%m-%d')\n",
    "    retframe['yyyymmdd'] = [convert_datestr(str) for str in retframe['yyyymmdd']]\n",
    "    retframe['DATETIME'] = pd.to_datetime(retframe['yyyymmdd'])\n",
    "    retframe.reset_index(inplace=True)\n",
    "    return retframe[[\"DATETIME\", series]]\n",
    "\n",
    "start_date = datetime.datetime(1963, 1, 1)\n",
    "end_date = datetime.datetime(2017, 12, 31)\n",
    "\n",
    "TB3MS = get_fred_data(\"TB3MS\", start_date, end_date)\n",
    "TB3MS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics \n",
    "\n",
    "def maxdrawdown(series):\n",
    "    \"\"\"max drawdown of a cumulative return series\"\"\"\n",
    "    return (series / series.cummax() -1).min()\n",
    "\n",
    "# todo: longest period of underperformance, worst vs.index\n",
    "\n",
    "def portmetrics_monthly(ret, rf):\n",
    "    \"given monthly return, monthly risk-free return as series, return portfolio metrics dataframe\"\n",
    "\n",
    "    annret = (1 + ret).cumprod().iloc[-1] ** (12/len(ret))-1\n",
    "    print(\"annualized return: %f\" % annret)\n",
    "\n",
    "    annvol = ret.std() * np.sqrt(12)\n",
    "    print(\"annualized volatility: %f\" % annvol)\n",
    "\n",
    "    #annualized excess return\n",
    "    annexcess = (1 + ret - rf).cumprod().iloc[-1] ** (12/len(ret))-1\n",
    "    print(\"annualized excess return: %f\" % annexcess)\n",
    "\n",
    "    # sharpe\n",
    "    sharpe = annexcess / annvol\n",
    "    print(\"Sharpe: %f\" % sharpe)\n",
    "\n",
    "    return pd.DataFrame({\"Metric\" : [\"Annualized return\",\"Annualized volatility\",\"Sharpe\"] , \n",
    "                         \"Value\": [annret, annvol, sharpe]})\n",
    "\n",
    "def sharpe(ret, rf):\n",
    "    \"given monthly return, monthly risk-free return as series, return Sharpe\"\n",
    "    annvol = ret.std() * np.sqrt(12)\n",
    "    annexcess_series = 1 + ret - rf\n",
    "    # cumulative excess return at end of series\n",
    "    annexcess = annexcess_series.cumprod().iloc[-1] ** (12/len(ret))-1    \n",
    "    sharpe = annexcess / annvol\n",
    "    return sharpe\n",
    "\n",
    "\n",
    "def turnover(data):\n",
    "    \"\"\"data has gvkeys, datetime, assumed equal weight, datedf = unique dates\"\"\"\n",
    "    datedf = data.groupby(\"DATETIME\").count().reset_index()[[\"DATETIME\",\"GVKEY\"]]\n",
    "    datedf[\"EQUALWEIGHT\"] = 1/datedf[\"GVKEY\"]\n",
    "    datedf.head()\n",
    "    \n",
    "    turnover_list = []\n",
    "    for i in range(1, len(datedf[\"DATETIME\"])):\n",
    "        date0 = datedf[\"DATETIME\"][i-1]\n",
    "        t0 = data.loc[data[\"DATETIME\"]==date0][[\"GVKEY\", \"EQUALWEIGHT\"]]\n",
    "\n",
    "        date1 = datedf[\"DATETIME\"][i]\n",
    "        t1 = data.loc[data[\"DATETIME\"]==date1][[\"GVKEY\", \"EQUALWEIGHT\"]]\n",
    "\n",
    "        turnover_df = t0.merge(t1, on=[\"GVKEY\"], how='outer',suffixes=[\"_t0\",\"_t1\"])    \n",
    "        turnover_df.fillna(0, inplace=True)\n",
    "        turnover_list.append(np.abs(turnover_df[\"EQUALWEIGHT_t0\"] - turnover_df[\"EQUALWEIGHT_t1\"]).sum()/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>GVKEY</th>\n",
       "      <th>IND_CODE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MOMENTUM</th>\n",
       "      <th>RET</th>\n",
       "      <th>RET3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1010</td>\n",
       "      <td>25</td>\n",
       "      <td>1.443624</td>\n",
       "      <td>0.126256</td>\n",
       "      <td>0.047002</td>\n",
       "      <td>0.077724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1040</td>\n",
       "      <td>6</td>\n",
       "      <td>0.448922</td>\n",
       "      <td>-0.477048</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.041148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4620</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1043</td>\n",
       "      <td>41</td>\n",
       "      <td>0.255952</td>\n",
       "      <td>-0.390406</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>-0.056931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1045</td>\n",
       "      <td>40</td>\n",
       "      <td>1.383475</td>\n",
       "      <td>-0.130926</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.082269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1075</td>\n",
       "      <td>31</td>\n",
       "      <td>0.476919</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>-0.002032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DATETIME  GVKEY  IND_CODE     VALUE  MOMENTUM       RET      RET3\n",
       "1016 1963-01-31   1010        25  1.443624  0.126256  0.047002  0.077724\n",
       "4326 1963-01-31   1040         6  0.448922 -0.477048  0.170732  0.041148\n",
       "4620 1963-01-31   1043        41  0.255952 -0.390406  0.049505 -0.056931\n",
       "5016 1963-01-31   1045        40  1.383475 -0.130926  0.102041  0.082269\n",
       "8419 1963-01-31   1075        31  0.476919 -0.187948  0.060082 -0.002032"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"data.pickle\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>GVKEY</th>\n",
       "      <th>IND_CODE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>MOMENTUM</th>\n",
       "      <th>RET</th>\n",
       "      <th>RET3</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1010</td>\n",
       "      <td>25</td>\n",
       "      <td>1.443624</td>\n",
       "      <td>0.126256</td>\n",
       "      <td>0.047002</td>\n",
       "      <td>0.077724</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1040</td>\n",
       "      <td>6</td>\n",
       "      <td>0.448922</td>\n",
       "      <td>-0.477048</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.041148</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1043</td>\n",
       "      <td>41</td>\n",
       "      <td>0.255952</td>\n",
       "      <td>-0.390406</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>-0.056931</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1045</td>\n",
       "      <td>40</td>\n",
       "      <td>1.383475</td>\n",
       "      <td>-0.130926</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.082269</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>1075</td>\n",
       "      <td>31</td>\n",
       "      <td>0.476919</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.060082</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DATETIME  GVKEY  IND_CODE     VALUE  MOMENTUM       RET      RET3  \\\n",
       "0 1963-01-31   1010        25  1.443624  0.126256  0.047002  0.077724   \n",
       "1 1963-01-31   1040         6  0.448922 -0.477048  0.170732  0.041148   \n",
       "2 1963-01-31   1043        41  0.255952 -0.390406  0.049505 -0.056931   \n",
       "3 1963-01-31   1045        40  1.383475 -0.130926  0.102041  0.082269   \n",
       "4 1963-01-31   1075        31  0.476919 -0.187948  0.060082 -0.002032   \n",
       "\n",
       "         RF  \n",
       "0  0.002425  \n",
       "1  0.002425  \n",
       "2  0.002425  \n",
       "3  0.002425  \n",
       "4  0.002425  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF=TB3MS.copy()\n",
    "RF[\"RF\"]= RF[\"TB3MS\"]/12/100\n",
    "data.merge(RF[[\"DATETIME\",\"RF\"]], on=\"DATETIME\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[numpy.datetime64('1963-01-31T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-02-28T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-03-31T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-04-30T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-05-31T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-06-30T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-07-31T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-08-31T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-09-30T00:00:00.000000000'),\n",
       " numpy.datetime64('1963-10-31T00:00:00.000000000')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniquedates = sorted(data[\"DATETIME\"].unique())\n",
    "uniquedates[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets = 5\n",
    "\n",
    "def bucket_df_by_col(df, col, num_buckets=num_buckets):\n",
    "    # compute quantile buckets for df.col\n",
    "    # assign new column col_RANK containing quantile bucket for each row\n",
    "    newcol = col + \"_RANK\"\n",
    "    # add a random small num to prevent bucket collisions\n",
    "    df[\"tempcol\"]=df[col] + np.random.uniform(0,0.0000001,len(df))\n",
    "    # bucket tempcol into num_buckets quantiles\n",
    "    vals = pd.qcut(df[\"tempcol\"], num_buckets, labels=False)\n",
    "    df[newcol] = vals\n",
    "    #df = df.assign(c=col.values)\n",
    "    return df\n",
    "\n",
    "col = \"MOMENTUM_RANK\"\n",
    "\n",
    "def perf_by_bucket(df, col, rank, perfcol=\"RET\"):\n",
    "    perf = df.loc[df[col]==rank][[\"DATETIME\", col, perfcol]]\\\n",
    "        .groupby(\"DATETIME\")\\\n",
    "        .mean()\\\n",
    "        .reset_index()\n",
    "    return perf\n",
    "\n",
    "def perf_bucket_by_col(data, col, nbuckets=5, rf=RF):\n",
    "\n",
    "    col_rank = col + \"_RANK\"\n",
    "\n",
    "    dflist = [bucket_df_by_col(data[data[\"DATETIME\"]==d], col, nbuckets) for d in uniquedates]\n",
    "    datanew= pd.concat(dflist)\n",
    "\n",
    "    reportdict = {'Label': [],\n",
    "                  'Annualized return' : [],\n",
    "                  'Annualized volatility' : [],\n",
    "                  'Sharpe': []}\n",
    "    retdf = None\n",
    "    for i in range(nbuckets):\n",
    "        \n",
    "        tempdf = perf_by_bucket(datanew, col_rank, i, perfcol=\"RET\")\n",
    "        # 1-month signal\n",
    "        tempdf[\"RET1P\"] = 1 + tempdf[\"RET\"]\n",
    "        tempdf[\"CUMPERF\"] = tempdf[\"RET1P\"].cumprod()\n",
    "        tempdf[\"RF\"] = rf[\"RF\"]\n",
    "        #display(tempdf)\n",
    "\n",
    "        finalcumreturn = list(tempdf[\"CUMPERF\"])[-1]\n",
    "        annret = (finalcumreturn**(12/len(tempdf))-1)*100\n",
    "        vol = tempdf[\"RET\"].std() * np.sqrt(12) *100\n",
    "\n",
    "        reportdict['Label'].append(\"Quintile %d \" % (i))\n",
    "        reportdict['Annualized return'].append(annret)\n",
    "        reportdict['Annualized volatility'].append(vol)\n",
    "        tempdf_sharpe = tempdf.dropna()\n",
    "        reportdict['Sharpe'].append(sharpe(tempdf_sharpe[\"RET\"], tempdf_sharpe[\"RF\"]))\n",
    "        \n",
    "        tempdf.set_index(\"DATETIME\", inplace=True)\n",
    "        plt.semilogy(tempdf['CUMPERF'], label=\"Quintile %d (%.1f%% p.a.)\" % (i, annret));\n",
    "        \n",
    "        if retdf is None:\n",
    "            retdf = tempdf\n",
    "        else:\n",
    "            retdf = pd.concat([retdf, tempdf])\n",
    "                 \n",
    "    plt.legend();\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "\n",
    "    with pd.option_context('display.float_format', lambda x: \"%.2f\" % x):\n",
    "        display(pd.DataFrame(reportdict))\n",
    "\n",
    "    return retdf\n",
    "  \n",
    "def perf_bucket_by_col_3(data, col, nbuckets=5, rf=RF):\n",
    "    \"\"\"Same but put 1/3 of port to work for 3 months\"\"\"\n",
    "    \n",
    "    dates_df=data[[\"DATETIME\"]].groupby([\"DATETIME\"]).count()\n",
    "    dates_df[\"t0\"]=dates_df.index\n",
    "    dates_df[\"t1\"]=dates_df.shift(-1)[\"t0\"]\n",
    "    dates_df[\"t2\"]=dates_df.shift(-2)[\"t0\"]\n",
    "\n",
    "    data[\"DATETIME1\"] = dates_df.loc[data[\"DATETIME\"]][\"t1\"].values\n",
    "    data[\"DATETIME2\"] = dates_df.loc[data[\"DATETIME\"]][\"t2\"].values    \n",
    "    \n",
    "    col_rank = col + \"_RANK\"\n",
    "\n",
    "    dflist = [bucket_df_by_col(data[data[\"DATETIME\"]==d], col, nbuckets) for d in uniquedates]\n",
    "    datanew= pd.concat(dflist)\n",
    "\n",
    "\n",
    "    # compute returns by bucket\n",
    "    reportdict = {'Label': [],\n",
    "                  'Annualized return' : [],\n",
    "                  'Annualized volatility' : [],\n",
    "                  'Sharpe': []}\n",
    "\n",
    "    for i in range(num_buckets):\n",
    "\n",
    "        # get returns for this bucket\n",
    "        T0 = datanew.loc[datanew[col_rank]==i]\n",
    "        # merge T0 on date +1 to get returns for month 2\n",
    "        T1 = T0[[\"DATETIME1\", \"GVKEY\"]].merge(datanew, \n",
    "                                              left_on=[\"DATETIME1\", \"GVKEY\"], \n",
    "                                              right_on=[\"DATETIME\", \"GVKEY\"])[[\"DATETIME\", \"GVKEY\", \"RET\"]]\n",
    "        # month 3\n",
    "        T2 = T0[[\"DATETIME2\", \"GVKEY\"]].merge(datanew, \n",
    "                                              left_on=[\"DATETIME2\", \"GVKEY\"], \n",
    "                                              right_on=[\"DATETIME\", \"GVKEY\"])[[\"DATETIME\", \"GVKEY\", \"RET\"]]\n",
    "        # compute means ... every month we put 1/3 portfolio to work for 3 months\n",
    "        G0 = T0[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "        G1 = T1[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "        G2 = T2[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "        # merge 3 portfolio returns\n",
    "        G_all = pd.merge(G1,G2,on=\"DATETIME\", how=\"outer\", suffixes=[\"1\", \"2\"])\n",
    "        G_all = pd.merge(G_all,G0,on=\"DATETIME\", how=\"outer\") \\\n",
    "            .reset_index() \\\n",
    "            .sort_values(\"DATETIME\") \\\n",
    "            .dropna()\n",
    "        # average returns by month\n",
    "        G_all[\"RET\"] = (G_all[\"RET\"] + G_all[\"RET1\"] + G_all[\"RET2\"]) /3\n",
    "        tempdf = pd.DataFrame(G_all[[\"DATETIME\", \"RET\"]])\n",
    "\n",
    "        tempdf[\"RET1P\"] = 1 + tempdf[\"RET\"]\n",
    "        tempdf[\"CUMPERF\"] = tempdf[\"RET1P\"].cumprod()\n",
    "        tempdf.reset_index(inplace=True)\n",
    "        tempdf[\"RF\"] = RF[\"RF\"]\n",
    "\n",
    "        lastval = list(tempdf[\"CUMPERF\"])[-1]\n",
    "        annret = (lastval**(12/len(tempdf))-1)*100\n",
    "        vol = tempdf[\"CUMPERF\"].pct_change().std() * np.sqrt(12) * 100\n",
    "\n",
    "        reportdict['Label'].append(\"Quintile %d \" % (i))\n",
    "        reportdict['Annualized return'].append(annret)\n",
    "        reportdict['Annualized volatility'].append(vol)\n",
    "        tempdf_sharpe = tempdf.dropna()\n",
    "        reportdict['Sharpe'].append(sharpe(tempdf_sharpe[\"RET\"], tempdf_sharpe[\"RF\"]))\n",
    "\n",
    "        tempdf.set_index(\"DATETIME\", inplace=True)\n",
    "        plt.semilogy(tempdf['CUMPERF'], label=\"Quintile %d (%.1f%% p.a.)\" % (i, annret));\n",
    "\n",
    "    plt.legend();\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "\n",
    "    with pd.option_context('display.float_format', lambda x: \"%.2f\" % x):\n",
    "        display(pd.DataFrame(reportdict))\n",
    "\n",
    "        # chart, calculate sharpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"VALUE\"\n",
    "value_returns = perf_bucket_by_col(data, col)\n",
    "value_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of RET3__RANK is VALUE_RANK\n",
    "# compute accuracy, confusion_matrix\n",
    "\n",
    "# compute rank for value and RET3\n",
    "bucket_df_by_col(data, \"VALUE\")\n",
    "bucket_df_by_col(data, \"RET\")\n",
    "\n",
    "acc = accuracy_score(data[\"RET_RANK\"], data[\"VALUE_RANK\"])\n",
    "print (\"Accuracy: %0.6f\" % acc)\n",
    "\n",
    "def conf_matrix_heatmap(y_xval, y_xval_pred, title=\"Xval confusion matrix\"):\n",
    "    conf_matrix = confusion_matrix(y_xval, y_xval_pred)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "    plt.ylabel('Actual Quantile')\n",
    "    plt.xlabel('Predicted Quantile')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "conf_matrix_heatmap(data[\"RET_RANK\"], data[\"VALUE_RANK\"], \"VALUE confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"MOMENTUM\"\n",
    "momentum_returns = perf_bucket_by_col(data, col)\n",
    "momentum_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of RET3__RANK is MOMENTUM_RANK\n",
    "# compute accuracy, confusion_matrix\n",
    "\n",
    "# make sure you have good rank for momentum and RET3\n",
    "bucket_df_by_col(data, \"MOMENTUM\")\n",
    "bucket_df_by_col(data, \"RET\")\n",
    "\n",
    "acc = accuracy_score(data[\"RET_RANK\"], data[\"MOMENTUM_RANK\"])\n",
    "print (\"Accuracy: %0.6f\" % acc)\n",
    "\n",
    "conf_matrix_heatmap(data[\"RET_RANK\"], data[\"MOMENTUM_RANK\"], \"MOMENTUM confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge value_returns and momentum_returns by date, bucket\n",
    "# compute metrics for combined portfolio\n",
    "\n",
    "combined_returns = value_returns.reset_index().merge(momentum_returns, \n",
    "                                                     left_on=[\"DATETIME\",\"VALUE_RANK\"], \n",
    "                                                     right_on=[\"DATETIME\", \"MOMENTUM_RANK\"])\n",
    "\n",
    "reportdict = {'Label': [],\n",
    "              'Annualized return' : [],\n",
    "              'Annualized volatility' : [],\n",
    "              'Sharpe': []}\n",
    "\n",
    "for i in range(num_buckets):\n",
    "        \n",
    "        tempdf = combined_returns.loc[combined_returns[\"VALUE_RANK\"] == i]\n",
    "        tempdf[\"RET1P\"]=(tempdf[\"RET1P_x\"] + tempdf[\"RET1P_y\"])/2\n",
    "        tempdf[\"RET1\"] = tempdf[\"RET1P\"]-1\n",
    "        tempdf[\"CUMPERF\"] = tempdf[\"RET1P\"].cumprod()\n",
    "        tempdf.reset_index(inplace=True)\n",
    "        tempdf[\"RF\"] = RF[\"RF\"]\n",
    "\n",
    "        lastval = list(tempdf[\"CUMPERF\"])[-1]\n",
    "\n",
    "        annret = (lastval**(12/len(tempdf))-1)*100\n",
    "        vol = tempdf[\"CUMPERF\"].pct_change().std() * np.sqrt(12) * 100\n",
    "\n",
    "        reportdict['Label'].append(\"Quintile %d \" % (i))\n",
    "        reportdict['Annualized return'].append(annret)\n",
    "        reportdict['Annualized volatility'].append(vol)\n",
    "        tempdf_sharpe = tempdf.dropna()\n",
    "        reportdict['Sharpe'].append(sharpe(tempdf_sharpe[\"RET1\"], tempdf_sharpe[\"RF\"]))\n",
    "\n",
    "        tempdf.set_index(\"DATETIME\", inplace=True)\n",
    "        plt.semilogy(tempdf['CUMPERF'], label=\"Quintile %d (%.1f%% p.a.)\" % (i, annret));\n",
    "\n",
    "        \n",
    "plt.legend();\n",
    "plt.title(\"COMBINED\")\n",
    "plt.show()\n",
    "\n",
    "with pd.option_context('display.float_format', lambda x: \"%.2f\" % x):\n",
    "    display(pd.DataFrame(reportdict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variable for financials, enable model to treat them differently, e.g. p/b\n",
    "data['FINANCIAL'] = 0\n",
    "data.loc[data[\"IND_CODE\"]==44,\"FINANCIAL\"] = 1\n",
    "data.loc[data[\"IND_CODE\"]==45,\"FINANCIAL\"] = 1\n",
    "data.loc[data[\"IND_CODE\"]==46,\"FINANCIAL\"] = 1\n",
    "data.loc[data[\"IND_CODE\"]==47,\"FINANCIAL\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 3-month return quintile buckets\n",
    "bucket_df_by_col(data, \"RET3\", num_buckets=num_buckets)\n",
    "   \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_TRAIN_MONTHS=120\n",
    "\n",
    "# BacktestModel training harness\n",
    "# initialize with data, sklearn model, some params\n",
    "# fit_predict: fit from start to some month, predict n months thereafter\n",
    "# gen_predictions: run fit_predict from some beginning month to end of dataframe using some stride\n",
    "\n",
    "class BacktestModel():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 df,\n",
    "                 create_model=None,\n",
    "                 startindex=FIRST_TRAIN_MONTHS,\n",
    "                 scaler=None,\n",
    "                 fit_missing=None):\n",
    "\n",
    "        # 1st column = dates\n",
    "        # 2nd column = gvkeys\n",
    "        self.data = df.copy()\n",
    "        self.data.sort_values([\"DATETIME\", \"GVKEY\"], inplace=True)\n",
    "        self.data.reset_index(inplace=True)\n",
    "        self.data[\"index\"] = self.data.index\n",
    "        self.uniquedates = self.data[[\"DATETIME\",\"index\"]].groupby(\"DATETIME\").first().reset_index()\n",
    "        self.uniquedatelist = list(self.uniquedates[\"index\"])\n",
    "        self.uniquedatedict = dict(zip(self.uniquedates[\"DATETIME\"], \n",
    "                                       self.uniquedates[\"index\"]))\n",
    "        # last column = target\n",
    "        self.y = df.iloc[:,-1]\n",
    "        \n",
    "        # middle = features\n",
    "        self.X = df.iloc[:,2:-1]\n",
    "        self.Xrows, self.Xcols = self.X.shape\n",
    "        \n",
    "        # create predictions\n",
    "        self.P = np.zeros(self.y.shape)\n",
    "        \n",
    "        self.Xscale = self.X.copy()\n",
    "        self.yscale = self.y.copy()\n",
    "\n",
    "        if scaler:\n",
    "            self.Xscale = scaler().fit_transform(self.Xscale)\n",
    "            #            self.yscale = scaler().fit_transform(self.yscale.values.reshape(self.Xrows,1))\n",
    "        \n",
    "        self.create_model = create_model\n",
    "        self.startindex = startindex\n",
    "        self.fit_missing = fit_missing\n",
    "        \n",
    "    def fit_predict(self, train_months, predict_months=1, verbose=False):\n",
    "        \"\"\"for backtest, train model  \n",
    "        train on first ntrain rows. if ntrain=121, fit 0:120\n",
    "        predict following npredict rows \n",
    "        if npredict=1, predict row 121\n",
    "        if npredict=12, predict rows 121-132\n",
    "        \"\"\"\n",
    "        \n",
    "        # fit first ntrain months\n",
    "        train_stop_index = self.uniquedatelist[train_months]\n",
    "        if verbose:\n",
    "            print(\"Training months 0:%d, indexes 0:%d\" % (train_months-1, train_stop_index-1))\n",
    "        X_fit = self.Xscale[:train_stop_index]  # e.g. 0:120\n",
    "        y_fit = self.yscale[:train_stop_index]\n",
    "\n",
    "        # train model\n",
    "        self.model = self.create_model()\n",
    "        self.model.fit(X_fit, y_fit)\n",
    "\n",
    "        # predict npredict months (but don't exceed bounds)\n",
    "        if train_months+predict_months > len(self.uniquedatelist)-1:\n",
    "            predict_stop_index = len(self.Xscale)\n",
    "        else:\n",
    "            predict_stop_index = self.uniquedatelist[train_months+predict_months]\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"Predicting months %d:%d, indexes %d:%d\" % (train_months, train_months+predict_months-1, \n",
    "                                                              train_stop_index, predict_stop_index-1))\n",
    "        self.X_predict = self.Xscale[train_stop_index:predict_stop_index] # 121-122\n",
    "        self.y_predict_proba = self.model.predict_proba(self.X_predict)\n",
    "        self.y_predict = self.y_predict_proba.dot(np.matrix(\"0 ; 1; 2; 3; 4\"))\n",
    "        self.P[train_stop_index:predict_stop_index] = self.y_predict.reshape(self.y_predict.shape[0])\n",
    "        return self.P\n",
    "    \n",
    "    def gen_predictions(self,\n",
    "                        step=1, \n",
    "                        splits=None,\n",
    "                        verbose=False):\n",
    "        \"\"\"predict all months, pass either step (# months) or total # of splits\"\"\"\n",
    "        print(\"%s Starting training\" % (strftime(\"%H:%M:%S\")))\n",
    "\n",
    "        if splits:\n",
    "            month_indexes = splits[:-1] # last index is nrows\n",
    "        else:\n",
    "            # create list of steps\n",
    "            month_indexes = list(range(self.startindex, len(self.uniquedatelist), step))\n",
    "            \n",
    "        steps = [month_indexes[i+1]-month_indexes[i] for i in range(len(month_indexes)-1)]\n",
    "        # last step -> end\n",
    "        steps.append(len(self.uniquedatelist) - month_indexes[-1])\n",
    "        \n",
    "        if verbose:\n",
    "            print (\"Months: \" + str(month_indexes))\n",
    "            print (\"Steps: \" + str(steps))\n",
    "\n",
    "        progress_i = 0\n",
    "            \n",
    "        for month_index, forecast_rows in zip(month_indexes, steps):\n",
    "            if verbose:\n",
    "                print(\"Training on first %d months (%d:%d), storing predictions in rows %s\" % (month_index, \n",
    "                                                                                            0, month_index-1, \n",
    "                                                                                            str(range(month_index,month_index+forecast_rows))))\n",
    "            predictions = self.fit_predict(month_index, forecast_rows, verbose=verbose)\n",
    "            sys.stdout.write('.')\n",
    "            progress_i += 1\n",
    "            if progress_i % 80 == 0:\n",
    "                print(\"\")\n",
    "                print(\"%s Still training step %d of %d\" % (strftime(\"%H:%M:%S\"), progress_i, len(month_indexes)))\n",
    "            sys.stdout.flush()\n",
    "        print(\"\")    \n",
    "        print(\"%s Finished\" % (strftime(\"%H:%M:%S\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_create_model():\n",
    "    return LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "backtestmodel = BacktestModel(data[[\"DATETIME\",\"GVKEY\",\"FINANCIAL\",\"VALUE\",\"MOMENTUM\",\"RET3_RANK\"]],\n",
    "                              create_model=my_create_model,\n",
    "                              startindex=FIRST_TRAIN_MONTHS)\n",
    "backtestmodel.gen_predictions(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"WALKFORWARD_LOGISTIC\"\n",
    "data[col] = backtestmodel.P\n",
    "perf_bucket_by_col_3(data, col)\n",
    "#tbh expected better performance than either predictor alone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNEFOLD_LENGTH = 6\n",
    "\n",
    "# tuned sequentially\n",
    "#n_estimators: number of base learner trees\n",
    "n_estimatorss=[50,100,150,200,300,500,1000]\n",
    "n_estimators = 100\n",
    "\n",
    "#max_depth: max depth per base tree\n",
    "#max_depths = range(3,16)\n",
    "max_depths = range(3,21)\n",
    "max_depth = 14\n",
    "\n",
    "#subsample: row subsampling rate (similar to RF)\n",
    "subsamples = np.linspace(0.75, 0.9, 4)\n",
    "subsample = 0.7\n",
    "\n",
    "#colsample_bytree: feature subsampling rate (similar to RF)\n",
    "#colsample_bytrees = np.linspace(0.4, 0.9, 11)\n",
    "# only 3 features though\n",
    "colsample_bytrees = [1/3, 2/3, 1.0]\n",
    "colsample_bytree = 1/3\n",
    "\n",
    "#learning_rate: shrinkage factor applied to each base tree update\n",
    "#learning_rates = np.logspace(-2, 0, 5)\n",
    "learning_rate = 0.1\n",
    "\n",
    "#gammas = [0, 1, 5]\n",
    "gamma = 1\n",
    "\n",
    "for colsample_bytree in colsample_bytrees:\n",
    "    \n",
    "    print(\"n_estimators: %d\" % n_estimators)\n",
    "    print(\"max_depth: %d\" % max_depth)\n",
    "    print(\"subsample: %f\" % subsample)\n",
    "    print(\"colsample_bytree: %f\" % colsample_bytree)\n",
    "    print(\"gamma: %f\" % gamma)\n",
    "    print(\"learning_rate: %f\" % learning_rate)\n",
    "    \n",
    "    def my_create_model():\n",
    "        return xgboost.sklearn.XGBClassifier(n_estimators=n_estimators, \n",
    "                                             max_depth=max_depth, \n",
    "                                             subsample=subsample,\n",
    "                                             min_child_weight=1,\n",
    "                                             colsample_bytree=colsample_bytree,\n",
    "                                             learning_rate=learning_rate,\n",
    "                                             gamma=gamma,\n",
    "                                             n_jobs=-1\n",
    "                                            )\n",
    "\n",
    "    backtestmodel = BacktestModel(data[[\"DATETIME\",\"GVKEY\",\"VALUE\",\"MOMENTUM\",\"FINANCIAL\",\"RET3_RANK\"]],\n",
    "                                  create_model=my_create_model,\n",
    "                                  startindex=FIRST_TRAIN_MONTHS)\n",
    "    backtestmodel.gen_predictions(verbose=False, step=TUNEFOLD_LENGTH)\n",
    "\n",
    "    col = \"WALKFORWARD_XGB_CLASSIFIER\"\n",
    "    data[col] = backtestmodel.P\n",
    "    z = perf_bucket_by_col(data, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 300\n",
    "max_depth = 14\n",
    "subsample = 0.7\n",
    "colsample_bytree = 1/3\n",
    "learning_rate = 0.1\n",
    "gamma = 0\n",
    "    \n",
    "print(\"n_estimators: %d\" % n_estimators)\n",
    "print(\"max_depth: %d\" % max_depth)\n",
    "print(\"subsample: %f\" % subsample)\n",
    "print(\"colsample_bytree: %f\" % colsample_bytree)\n",
    "print(\"gamma: %f\" % gamma)\n",
    "print(\"learning_rate: %f\" % learning_rate)\n",
    "\n",
    "def my_create_model():\n",
    "    return xgboost.sklearn.XGBClassifier(n_estimators=n_estimators, \n",
    "                                         max_depth=max_depth, \n",
    "                                         subsample=subsample,\n",
    "                                         min_child_weight=1,\n",
    "                                         colsample_bytree=colsample_bytree,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         gamma=gamma,\n",
    "                                         n_jobs=-1\n",
    "                                        )\n",
    "\n",
    "backtestmodel = BacktestModel(data[[\"DATETIME\",\"GVKEY\",\"VALUE\",\"MOMENTUM\",\"FINANCIAL\",\"RET3_RANK\"]],\n",
    "                              create_model=my_create_model,\n",
    "                              startindex=FIRST_TRAIN_MONTHS)\n",
    "backtestmodel.gen_predictions(verbose=False, step=1)\n",
    "\n",
    "col = \"WALKFORWARD_XGB_CLASSIFIER\"\n",
    "data[col] = backtestmodel.P\n",
    "perf_bucket_by_col_3(data, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateindexes[120]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_confmat = data.copy()\n",
    "data_confmat[\"WALKFORWARD_XGB\"] = backtestmodel.P\n",
    "data_confmat = data_confmat[dateindexes[120]:].copy()\n",
    "\n",
    "# compute rank for pred and RET3\n",
    "bucket_df_by_col(data_confmat, \"WALKFORWARD_XGB\")\n",
    "bucket_df_by_col(data_confmat, \"RET3\")\n",
    "\n",
    "acc = accuracy_score(data_confmat[\"RET3_RANK\"], data_confmat[\"WALKFORWARD_XGB_RANK\"])\n",
    "print (\"Accuracy: %0.6f\" % acc)\n",
    "\n",
    "conf_matrix_heatmap(data_confmat[\"RET3_RANK\"], data_confmat[\"WALKFORWARD_XGB_RANK\"], \"Gradient Boosting Classifier Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['FINANCIAL' 'RET3_RANK'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-852a21b05e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DATETIME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GVKEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"level_0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"DATETIME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"GVKEY\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"FINANCIAL\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"VALUE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"MOMENTUM\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"RET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"RET3_RANK\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"RET3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"level_0\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['FINANCIAL' 'RET3_RANK'] not in index\""
     ]
    }
   ],
   "source": [
    "# rewrite backtest in more brain-dead way to make sure no error \n",
    "n_estimators = 300\n",
    "max_depth = 14\n",
    "subsample = 0.7\n",
    "colsample_bytree = 1/3\n",
    "learning_rate = 0.1\n",
    "gamma = 0\n",
    "\n",
    "# make sure sorted by date, gvkey\n",
    "data2 = data.sort_values([\"DATETIME\", \"GVKEY\"]) \\\n",
    "    .reset_index()\\\n",
    "    .reset_index()[[\"level_0\",\"DATETIME\",\"GVKEY\",\"FINANCIAL\",\"VALUE\",\"MOMENTUM\",\"RET\",\"RET3_RANK\",\"RET3\"]]\n",
    "data2.rename(index=str, columns={\"level_0\": \"index\"}, inplace=True)\n",
    "\n",
    "# map dates to first row\n",
    "dateindexes = data2.groupby([\"DATETIME\"])[\"index\"].first()\n",
    "\n",
    "P = np.zeros(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dateindexes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9f486e6fff63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdaterange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdateindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit from %s to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaterange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaterange\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfitrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdateindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dateindexes' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(120, len(dateindexes)):\n",
    "    \n",
    "    daterange = dateindexes.index[0:i]\n",
    "    print(\"fit from %s to %s\" % (str(daterange[0]), str(daterange[-1])))\n",
    "    fitrange = data2.iloc[0:dateindexes[i],:]\n",
    "    print(\"fit from index %d to index %d\" % (fitrange.iloc[0,:][\"index\"], fitrange.iloc[-1,:][\"index\"]))\n",
    "    X = fitrange[[\"VALUE\",\"MOMENTUM\",\"FINANCIAL\"]]\n",
    "    y = fitrange[\"RET3_RANK\"]\n",
    "    \n",
    "#     model = LogisticRegression(multi_class='multinomial', \n",
    "#                              solver='lbfgs', \n",
    "#                              max_iter=10000,\n",
    "#                              n_jobs=-1)\n",
    "    model = xgboost.sklearn.XGBClassifier(n_estimators=n_estimators, \n",
    "                                        max_depth=max_depth, \n",
    "                                        subsample=subsample,\n",
    "                                        min_child_weight=1,\n",
    "                                        colsample_bytree=colsample_bytree,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        gamma=gamma,\n",
    "                                        n_jobs=-1\n",
    "                                       )\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    print(\"predict %s\" % (dateindexes.index[i]))\n",
    "    if i+1 >= len(dateindexes): # last start date, predict from there to the end\n",
    "        predrange = data2.iloc[dateindexes[i]:,:]\n",
    "    else:\n",
    "        predrange = data2.iloc[dateindexes[i]:dateindexes[i+1],:]\n",
    "    print(\"predict from index %d to index %d\" % (predrange.iloc[0,:][\"index\"], predrange.iloc[-1,:][\"index\"]))\n",
    "    X_predict = predrange[[\"VALUE\",\"MOMENTUM\",\"FINANCIAL\"]]\n",
    "    y_predict_proba = model.predict_proba(X_predict)\n",
    "    # weighted average \n",
    "    y_predict = y_predict_proba.dot(np.matrix(\"0 ; 1; 2; 3; 4\"))\n",
    "    if i+1 >= len(dateindexes):\n",
    "        P[dateindexes[i]:] = y_predict.reshape(y_predict.shape[0])\n",
    "    else:\n",
    "        P[dateindexes[i]:dateindexes[i+1]] = y_predict.reshape(y_predict.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateindexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have forecasts for full period\n",
    "# bucket forecasts\n",
    "\n",
    "# put P forecast into data2\n",
    "data2[\"P\"] = P  \n",
    "\n",
    "# bucket predictions, put into PREDRANK\n",
    "data3 = None\n",
    "for d in dateindexes.index[120:]:\n",
    "    tempdf = data2.loc[data2[\"DATETIME\"]==d]\n",
    "    print(d, len(tempdf))\n",
    "    tempdf[\"PREDRANK\"] = pd.qcut(tempdf[\"P\"], num_buckets, labels=False)\n",
    "    if data3 is None:\n",
    "        data3 = tempdf\n",
    "    else:\n",
    "        data3 = pd.concat([data3, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that we predicted 3-month returns\n",
    "# for all the 1s, we will collect this month's return + next 2 months, to hold for 3 months [datetime, gvkey, return, monthno]\n",
    "# for now, store the date values as extra columns in data3\n",
    "\n",
    "dates_df = pd.DataFrame(dateindexes)\n",
    "dates_df[\"t0\"]=dates_df.index\n",
    "dates_df[\"t1\"]=dates_df.shift(-1)[\"t0\"]\n",
    "dates_df[\"t2\"]=dates_df.shift(-2)[\"t0\"]\n",
    "\n",
    "data3[\"DATETIME1\"] = dates_df.loc[data3[\"DATETIME\"]][\"t1\"].values\n",
    "data3[\"DATETIME2\"] = dates_df.loc[data3[\"DATETIME\"]][\"t2\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute returns by bucket\n",
    "# calculate sharpe\n",
    "\n",
    "reportdict = {'Label': [],\n",
    "              'Annualized return' : [],\n",
    "              'Annualized volatility' : [],\n",
    "              'Sharpe': []}\n",
    "\n",
    "for i in range(num_buckets):\n",
    "\n",
    "    # get returns for this bucket\n",
    "    T0 = data3.loc[data3[\"PREDRANK\"]==i]\n",
    "    # merge T0 on date +1 to get returns for month 2\n",
    "    T1 = T0[[\"DATETIME1\", \"GVKEY\"]].merge(data3, \n",
    "                                          left_on=[\"DATETIME1\", \"GVKEY\"], \n",
    "                                          right_on=[\"DATETIME\", \"GVKEY\"])[[\"DATETIME\", \"GVKEY\", \"RET\"]]\n",
    "    # month 3\n",
    "    T2 = T0[[\"DATETIME2\", \"GVKEY\"]].merge(data3, \n",
    "                                          left_on=[\"DATETIME2\", \"GVKEY\"], \n",
    "                                          right_on=[\"DATETIME\", \"GVKEY\"])[[\"DATETIME\", \"GVKEY\", \"RET\"]]\n",
    "    # compute means ... every month we put 1/3 portfolio to work for 3 months\n",
    "    G0 = T0[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "    G1 = T1[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "    G2 = T2[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "    # merge 3 portfolio returns\n",
    "    G_all = pd.merge(G1,G2,on=\"DATETIME\", how=\"outer\", suffixes=[\"1\", \"2\"])\n",
    "    G_all = pd.merge(G_all,G0,on=\"DATETIME\", how=\"outer\") \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(\"DATETIME\") \\\n",
    "        .dropna()\n",
    "    # average returns by month\n",
    "    G_all[\"RET\"] = (G_all[\"RET\"] + G_all[\"RET1\"] + G_all[\"RET2\"]) /3\n",
    "    tempdf = pd.DataFrame(G_all[[\"DATETIME\", \"RET\"]])\n",
    "\n",
    "    tempdf[\"RET1P\"] = 1 + tempdf[\"RET\"]\n",
    "    tempdf[\"CUMPERF\"] = tempdf[\"RET1P\"].cumprod()\n",
    "    tempdf.reset_index(inplace=True)\n",
    "    tempdf[\"RF\"] = RF[\"RF\"]\n",
    "\n",
    "    lastval = list(tempdf[\"CUMPERF\"])[-1]\n",
    "    annret = (lastval**(12/len(tempdf))-1)*100\n",
    "    vol = tempdf[\"CUMPERF\"].pct_change().std() * np.sqrt(12) * 100\n",
    "\n",
    "    reportdict['Label'].append(\"Quintile %d \" % (i))\n",
    "    reportdict['Annualized return'].append(annret)\n",
    "    reportdict['Annualized volatility'].append(vol)\n",
    "    tempdf_sharpe = tempdf.dropna()\n",
    "    reportdict['Sharpe'].append(sharpe(tempdf_sharpe[\"RET\"], tempdf_sharpe[\"RF\"]))\n",
    "\n",
    "    tempdf.set_index(\"DATETIME\", inplace=True)\n",
    "    plt.semilogy(tempdf['CUMPERF'], label=\"Quintile %d (%.1f%% p.a.)\" % (i, annret));\n",
    "\n",
    "plt.legend();\n",
    "plt.title(\"XGBCLASSIFIER\")\n",
    "plt.show()\n",
    "\n",
    "with pd.option_context('display.float_format', lambda x: \"%.2f\" % x):\n",
    "    display(pd.DataFrame(reportdict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    G0 = T0[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "    G1 = T1[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "    G2 = T2[[\"DATETIME\",\"RET\"]].groupby(\"DATETIME\").mean()\n",
    "    G_all = pd.merge(G1,G2,on=\"DATETIME\", how=\"outer\", suffixes=[\"1\", \"2\"])\n",
    "    G_all = pd.merge(G_all,G0,on=\"DATETIME\", how=\"outer\") \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(\"DATETIME\") \\\n",
    "        .dropna()\n",
    "    G_all[\"RET_COMB\"] = (G_all[\"RET\"] + G_all[\"RET1\"] + G_all[\"RET2\"]) /3\n",
    "    G_all[\"RET\"] = G_all[\"RET_COMB\"]\n",
    "    tempdf = pd.DataFrame(G_all[[\"DATETIME\", \"RET\"]])\n",
    "    tempdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "col = \"XGB_CLASSIFIER\"\n",
    "data[col] = backtestmodel.P\n",
    "z = perf_bucket_by_col(data, col)\n",
    "display(HTML(z.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of RET3_RANK is VALUE_RANK\n",
    "# compute accuracy, confusion_matrix\n",
    "\n",
    "print (len(data)/25)\n",
    "# make sure p is in data\n",
    "bucket_df_by_col(data, col)\n",
    "bucket_df_by_col(data, \"RET3\")\n",
    "\n",
    "acc = accuracy_score(data[\"RET3_RANK\"], data[col+\"_RANK\"])\n",
    "print (\"Accuracy: %0.6f\" % acc)\n",
    "\n",
    "conf_matrix_heatmap(data[\"RET3_RANK\"], data[col + \"_RANK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "import operator\n",
    "\n",
    "featimpdict = backtestmodel.model._Booster.get_score(importance_type='gain')\n",
    "\n",
    "xgb_featlabels=[]\n",
    "xgb_featimps = []\n",
    "\n",
    "for key, val in reversed(sorted(featimpdict.items(), key=operator.itemgetter(1))):\n",
    "    xgb_featlabels.append(key)\n",
    "    xgb_featimps.append(val)\n",
    "\n",
    "def plotly_feat_imp(labels, featimps, name):\n",
    "\n",
    "    trace1 = Bar(\n",
    "        x=labels[:30],\n",
    "        y=featimps[:30],\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    zlayout = Layout(\n",
    "        title='%s Feature Importance' % (name),        \n",
    "        xaxis = dict(title = \"\", \n",
    "                     tickangle=30,\n",
    "                     tickfont=dict(\n",
    "                         size=10,\n",
    "                         color='black'\n",
    "                     )),\n",
    "        yaxis = dict(title = \"Importance\", \n",
    "                    ),\n",
    "        \n",
    "        margin=layout.Margin(\n",
    "            b=150,\n",
    "        ),    \n",
    "        barmode='group',\n",
    "    )\n",
    "\n",
    "    data = [trace1]\n",
    "    \n",
    "    fig = Figure(data=data, layout=zlayout)\n",
    "    iplot(fig)\n",
    "\n",
    "plotly_feat_imp(xgb_featlabels, xgb_featimps, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite backtest in more brain-dead way to make sure no error \n",
    "n_estimators = 300\n",
    "max_depth = 14\n",
    "subsample = 0.7\n",
    "colsample_bytree = 1/3\n",
    "learning_rate = 0.1\n",
    "gamma = 0\n",
    "\n",
    "# make sure sorted by date, gvkey\n",
    "data2 = data.sort_values([\"DATETIME\", \"GVKEY\"]) \\\n",
    "    .reset_index()\\\n",
    "    .reset_index()[[\"level_0\",\"DATETIME\",\"GVKEY\",\"FINANCIAL\",\"VALUE\",\"MOMENTUM\",\"RET\",\"RET3_RANK\",\"RET3\"]]\n",
    "data2.rename(index=str, columns={\"level_0\": \"index\"}, inplace=True)\n",
    "\n",
    "# map dates to first row\n",
    "dateindexes = data2.groupby([\"DATETIME\"])[\"index\"].first()\n",
    "\n",
    "P = np.zeros(len(data2))\n",
    "\n",
    "for i in range(120, len(dateindexes)):\n",
    "    \n",
    "    daterange = dateindexes.index[0:i]\n",
    "    print(\"fit from %s to %s\" % (str(daterange[0]), str(daterange[-1])))\n",
    "    fitrange = data2.iloc[0:dateindexes[i],:]\n",
    "    print(\"fit from index %d to index %d\" % (fitrange.iloc[0,:][\"index\"], fitrange.iloc[-1,:][\"index\"]))\n",
    "    X = fitrange[[\"VALUE\",\"MOMENTUM\",\"FINANCIAL\"]]\n",
    "    y = fitrange[\"RET3_RANK\"]\n",
    "    \n",
    "#     model = LogisticRegression(multi_class='multinomial', \n",
    "#                              solver='lbfgs', \n",
    "#                              max_iter=10000,\n",
    "#                              n_jobs=-1)\n",
    "    model = xgboost.sklearn.XGBClassifier(n_estimators=n_estimators, \n",
    "                                        max_depth=max_depth, \n",
    "                                        subsample=subsample,\n",
    "                                        min_child_weight=1,\n",
    "                                        colsample_bytree=colsample_bytree,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        gamma=gamma,\n",
    "                                        n_jobs=-1\n",
    "                                       )\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    print(\"predict %s\" % (dateindexes.index[i]))\n",
    "    if i+1 >= len(dateindexes): # last start date, predict from there to the end\n",
    "        predrange = data2.iloc[dateindexes[i]:,:]\n",
    "    else:\n",
    "        predrange = data2.iloc[dateindexes[i]:dateindexes[i+1],:]\n",
    "    print(\"predict from index %d to index %d\" % (predrange.iloc[0,:][\"index\"], predrange.iloc[-1,:][\"index\"]))\n",
    "    X_predict = predrange[[\"VALUE\",\"MOMENTUM\",\"FINANCIAL\"]]\n",
    "    y_predict_proba = model.predict_proba(X_predict)\n",
    "    # weighted average \n",
    "    y_predict = y_predict_proba.dot(np.matrix(\"0 ; 1; 2; 3; 4\"))\n",
    "    if i+1 >= len(dateindexes):\n",
    "        P[dateindexes[i]:] = y_predict.reshape(y_predict.shape[0])\n",
    "    else:\n",
    "        P[dateindexes[i]:dateindexes[i+1]] = y_predict.reshape(y_predict.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(backtestmodel.startindex, len(backtestmodel.uniquedatelist), 108))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "120\n",
    "len(uniquedates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(658-120)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(16,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        mydata = data.copy()\n",
    "        mydata.sort_values([\"DATETIME\", \"GVKEY\"], inplace=True)\n",
    "        mydata.reset_index(inplace=True)\n",
    "        mydata[\"index\"] = mydata.index\n",
    "        myuniquedates = mydata.groupby(\"DATETIME\").index.first().reset_index()\n",
    "        myuniquedatelist = list(myuniquedates[\"index\"])\n",
    "        myuniquedatedict = dict(zip(myuniquedates[\"DATETIME\"], \n",
    "                                       myuniquedates[\"index\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myuniquedates = mydata.groupby(\"DATETIME\").index.first().reset_index()\n",
    "myuniquedates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
